\chapter{Conclusion}

Different methods of preserving data privacy during the training of Machine Learning models have been studied. The aim has been to have a general overview of what is being used in practice and what challenges and drawbacks each method has.\\
Federated Learning makes sense in particular cases where different entities want to collaborate to improve models using data they could not have obtained otherwise. However, if the training and data distribution are not supervised, the resulting model may not perform as expected. In other cases, it is not necessary to train neural network models, and simpler models like decision trees or XGBoosting may be more appropriate. For these cases, there are cryptographic protocols within the field of Multi-Party Computation that allow for secure and private training without significant performance loss, albeit at the cost of increased computational and communication overhead.\\
It has been seen that implementing a SplitNN model can make sense in cases where the data has a vertical partition, thanks to protocols such as Private Set Intersection. Other VFL models require the use of cryptographic protocols or MPC. Again, this solves particular cases, not just any problem where the data follows this type of partition.\\
Methods like Differential Privacy also have use cases, such as obtaining statistical metrics by querying databases or enhancing the security of deep learning models during the optimization step. However, this comes at a performance cost to the model, which must be adjusted according to the desired level of privacy. Generative AI, despite its surge in recent years, does not immediately solve the privacy problem either, as it is known that original data can be obtained from synthetic samples. It can be useful in specific cases, understanding the privacy guarantees sought.\\

Data generates money and value, and thus there will always be an interest in exploiting it. The fact that public institutions are concerned with regulating and punishing practices that circumvent indiscriminate use of data is an indicator that there is a growing concern about the use of personal data to feed machine learning models, often privately in companies and institutions that are not transparent about their practices. Industries and universities must continue exploring different techniques to preserve the privacy of citizens. Still, we must contextualize each technique with its use case and understand the tools we are working with.\\

In addition, we must not only be concerned with the protection of the data with which they are trained, but we must also study the possible discriminatory biases they may have, the robustness, and the explainability of these models. All these challenges will not be solved with a revolutionary algorithm that fixes data privacy, model security, or biases. These are complex issues that need to be studied from diverse perspectives in collaborative environments, as they involve not only technical problems but also social and legislative ones. We must not impose a technical solution on a poorly conceived problem.\\
The technologies presented here are promising but not magical. They require research, careful implementation, and supervision. They are not free from criticism and possible security flaws in certain cases, in addition to a potential degradation of models compared to centralized training. The aim is not to present these topics as the solution to privacy, but rather to understand, even if superficially, what they are about and what they seek to solve. I hope I have achieved this, or at least provided a good bibliography to consult material on each topic.\\
