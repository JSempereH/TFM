@inproceedings{abadi2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Abadi, Mart{\'i}n and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  month = oct,
  eprint = {1607.00133},
  primaryclass = {cs, stat},
  pages = {308--318},
  doi = {10.1145/2976749.2978318},
  urldate = {2024-07-21},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  archiveprefix = {arXiv},
  file = {/home/javier/Zotero/storage/B3DN6FNY/1607.html}
}

@article{agrawal,
  title = {Information {{Sharing Across Private Databases}}},
  author = {Agrawal, Rakesh and Evfimievski, Alexandre and Srikant, Ramakrishnan},
  abstract = {Literature on information integration across databases tacitly assumes that the data in each database can be revealed to the other databases. However, there is an increasing need for sharing information across autonomous entities in such a way that no information apart from the answer to the query is revealed. We formalize the notion of minimal information sharing across private databases, and develop protocols for intersection, equijoin, intersection size, and equijoin size. We also show how new applications can be built using the proposed protocols.},
  langid = {english},
  file = {/home/javier/Zotero/storage/7GE9EUV3/Agrawal et al. - Information Sharing Across Private Databases.pdf}
}

@book{bishop2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  file = {/home/javier/Zotero/storage/N2QAVMF2/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@misc{bureau,
  title = {Why the {{Census Bureau Chose Differential Privacy}}},
  author = {Bureau, US Census},
  journal = {Census.gov},
  urldate = {2024-07-18},
  abstract = {This brief describing how disclosure avoidance methods are being applied to 2020 Census data products and implications of those methods for data users.},
  chapter = {Government},
  howpublished = {https://www.census.gov/library/publications/2023/decennial/c2020br-03.html}
}

@misc{caldas2019,
  title = {{{LEAF}}: {{A Benchmark}} for {{Federated Settings}}},
  shorttitle = {{{LEAF}}},
  author = {Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v c}n{\'y}, Jakub and McMahan, H. Brendan and Smith, Virginia and Talwalkar, Ameet},
  year = {2019},
  month = dec,
  number = {arXiv:1812.01097},
  eprint = {1812.01097},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-03},
  abstract = {Modern federated networks, such as those comprised of wearable devices, mobile phones, or autonomous vehicles, generate massive amounts of data each day. This wealth of data can help to learn models that can improve the user experience on each device. However, the scale and heterogeneity of federated data presents new challenges in research areas such as federated learning, meta-learning, and multi-task learning. As the machine learning community begins to tackle these challenges, we are at a critical time to ensure that developments made in these areas are grounded with realistic benchmarks. To this end, we propose LEAF, a modular benchmarking framework for learning in federated settings. LEAF includes a suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/6ZC7FA2G/Caldas et al. - 2019 - LEAF A Benchmark for Federated Settings.pdf}
}

@book{calin2020,
  title = {Deep {{Learning Architectures}}: {{A Mathematical Approach}}},
  shorttitle = {Deep {{Learning Architectures}}},
  author = {Calin, Ovidiu},
  year = {2020},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-36721-3},
  urldate = {2024-05-23},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-030-36720-6 978-3-030-36721-3},
  langid = {english},
  file = {/home/javier/Zotero/storage/3RFUWDFU/Calin - 2020 - Deep Learning Architectures A Mathematical Approa.pdf}
}

@misc{damadi2023,
  title = {The {{Backpropagation}} Algorithm for a Math Student},
  author = {Damadi, Saeed and Moharrer, Golnaz and Cham, Mostafa},
  year = {2023},
  month = may,
  number = {arXiv:2301.09977},
  eprint = {2301.09977},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-05-26},
  abstract = {A Deep Neural Network (DNN) is a composite function of vector-valued functions, and in order to train a DNN, it is necessary to calculate the gradient of the loss function with respect to all parameters. This calculation can be a non-trivial task because the loss function of a DNN is a composition of several nonlinear functions, each with numerous parameters. The Backpropagation (BP) algorithm leverages the composite structure of the DNN to efficiently compute the gradient. As a result, the number of layers in the network does not significantly impact the complexity of the calculation. The objective of this paper is to express the gradient of the loss function in terms of a matrix multiplication using the Jacobian operator. This can be achieved by considering the total derivative of each layer with respect to its parameters and expressing it as a Jacobian matrix. The gradient can then be represented as the matrix product of these Jacobian matrices. This approach is valid because the chain rule can be applied to a composition of vector-valued functions, and the use of Jacobian matrices allows for the incorporation of multiple inputs and outputs. By providing concise mathematical justifications, the results can be made understandable and useful to a broad audience from various disciplines.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/AT3GAHGP/Damadi et al. - 2023 - The Backpropagation algorithm for a math student.pdf}
}

@misc{dauphin2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2572},
  eprint = {1406.2572},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/7QCEKUB7/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf}
}

@article{deng2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  urldate = {2024-07-18},
  abstract = {In this issue, ``Best of the Web'' presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  file = {/home/javier/Zotero/storage/2RVI7BVG/6296535.html}
}

@article{dwork2013,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2013},
  journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume = {9},
  number = {3-4},
  pages = {211--407},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  urldate = {2024-07-18},
  langid = {english},
  file = {/home/javier/Zotero/storage/X9UQ3UBZ/Dwork and Roth - 2013 - The Algorithmic Foundations of Differential Privac.pdf}
}

@misc{escudero2022,
  title = {An {{Introduction}} to {{Secret-Sharing-Based Secure Multiparty Computation}}},
  author = {Escudero, Daniel},
  year = {2022},
  number = {2022/062},
  urldate = {2024-07-01},
  abstract = {This text serves as a general guide to secure multiparty computation based on secret-sharing, focusing more on practical aspects of the techniques and constructions rather than their theoretical grounds. It is intended to serve as an introductory reference text for readers interested in the area, assuming essentially no background in these topics. This work in progress currently includes an introduction to several core concepts in secure multiparty computation, an overview of simulation-based security, and detailed constructions for honest and two-thirds honest majority MPC, and also dishonest majority in the preprocessing model.},
  annotation = {Publication info: Preprint.},
  file = {/home/javier/Zotero/storage/RCU454DE/Escudero - 2022 - An Introduction to Secret-Sharing-Based Secure Mul.pdf}
}

@inproceedings{fredrikson2015,
  title = {Model {{Inversion Attacks}} That {{Exploit Confidence Information}} and {{Basic Countermeasures}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  year = {2015},
  month = oct,
  pages = {1322--1333},
  publisher = {ACM},
  address = {Denver Colorado USA},
  doi = {10.1145/2810103.2813677},
  urldate = {2024-07-20},
  abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
  isbn = {978-1-4503-3832-5},
  langid = {english},
  file = {/home/javier/Zotero/storage/QP44QBGP/Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf}
}

@inproceedings{glorot2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2024-05-23},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  langid = {english},
  file = {/home/javier/Zotero/storage/VSVF7YY6/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf}
}

@misc{goodfellow2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/MYCMBECN/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf}
}

@book{goodfellow2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016}
}

@misc{gupta2018,
  title = {Distributed Learning of Deep Neural Network over Multiple Agents},
  author = {Gupta, Otkrist and Raskar, Ramesh},
  year = {2018},
  month = oct,
  number = {arXiv:1810.06060},
  eprint = {1810.06060},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-17},
  abstract = {In domains such as health care and finance, shortage of labeled data and computational resources is a critical issue while developing machine learning algorithms. To address the issue of labeled data scarcity in training and deployment of neural network-based systems, we propose a new technique to train deep neural networks over several data sources. Our method allows for deep neural networks to be trained using data from multiple entities in a distributed fashion. We evaluate our algorithm on existing datasets and show that it obtains performance which is similar to a regular neural network trained on a single machine. We further extend it to incorporate semi-supervised learning when training with few labeled samples, and analyze any security concerns that may arise. Our algorithm paves the way for distributed training of deep neural networks in data sensitive applications when raw data may not be shared directly.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/C482SX8G/1810.pdf}
}

@article{jordon2019,
  title = {{{PATE-GAN}}: {{GENERATING SYNTHETIC DATA WITH DIFFERENTIAL PRIVACY GUARANTEES}}},
  author = {Jordon, James and Yoon, Jinsung},
  year = {2019},
  abstract = {Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the stateof-the-art method with respect to this and other notions of synthetic data quality.},
  langid = {english},
  file = {/home/javier/Zotero/storage/GEMI26U7/Jordon and Yoon - 2019 - PATE-GAN GENERATING SYNTHETIC DATA WITH DIFFERENT.pdf}
}

@misc{kairouz2021,
  title = {Advances and {{Open Problems}} in {{Federated Learning}}},
  author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gasc{\'o}n, Adri{\`a} and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Kone{\v c}n{\'y}, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancr{\`e}de and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and {\"O}zg{\"u}r, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tram{\`e}r, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
  year = {2021},
  month = mar,
  number = {arXiv:1912.04977},
  eprint = {1912.04977},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-15},
  abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/SYCUPYBQ/Kairouz et al. - 2021 - Advances and Open Problems in Federated Learning.pdf}
}

@misc{kairouz2021a,
  title = {Advances and {{Open Problems}} in {{Federated Learning}}},
  author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gasc{\'o}n, Adri{\`a} and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Kone{\v c}n{\'y}, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancr{\`e}de and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and {\"O}zg{\"u}r, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tram{\`e}r, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
  year = {2021},
  month = mar,
  number = {arXiv:1912.04977},
  eprint = {1912.04977},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-30},
  abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/43V86CIJ/Kairouz et al. - 2021 - Advances and Open Problems in Federated Learning.pdf}
}

@misc{karimireddy2021,
  title = {{{SCAFFOLD}}: {{Stochastic Controlled Averaging}} for {{Federated Learning}}},
  shorttitle = {{{SCAFFOLD}}},
  author = {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J. and Stich, Sebastian U. and Suresh, Ananda Theertha},
  year = {2021},
  month = apr,
  number = {arXiv:1910.06378},
  eprint = {1910.06378},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-06-29},
  abstract = {Federated Averaging (FEDAVG) has emerged as the algorithm of choice for federated learning due to its simplicity and low communication cost. However, in spite of recent research efforts, its performance is not fully understood. We obtain tight convergence rates for FEDAVG and prove that it suffers from `client-drift' when the data is heterogeneous (non-iid), resulting in unstable and slow convergence.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/BT7U6ME8/Karimireddy et al. - 2021 - SCAFFOLD Stochastic Controlled Averaging for Fede.pdf}
}

@misc{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-26},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/K6U7DNVH/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@article{krizhevsky2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2024-05-23},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/javier/Zotero/storage/IQRNZVQT/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@misc{li2020,
  title = {Federated {{Optimization}} in {{Heterogeneous Networks}}},
  author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  year = {2020},
  month = apr,
  number = {arXiv:1812.06127},
  eprint = {1812.06127},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-16},
  abstract = {Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22\% on average.},
  archiveprefix = {arXiv},
  file = {/home/javier/Zotero/storage/9D2E5TRQ/Li et al. - 2020 - Federated Optimization in Heterogeneous Networks.pdf;/home/javier/Zotero/storage/DGCZVR7F/1812.html}
}

@misc{li2021,
  title = {Federated {{Learning}} on {{Non-IID Data Silos}}: {{An Experimental Study}}},
  shorttitle = {Federated {{Learning}} on {{Non-IID Data Silos}}},
  author = {Li, Qinbin and Diao, Yiqun and Chen, Quan and He, Bingsheng},
  year = {2021},
  month = oct,
  number = {arXiv:2102.02079},
  eprint = {2102.02079},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Due to the increasing privacy concerns and data regulations, training data have been increasingly fragmented, forming distributed databases of multiple "data silos" (e.g., within different organizations and countries). To develop effective machine learning services, there is a must to exploit data from such distributed databases without exchanging the raw data. Recently, federated learning (FL) has been a solution with growing interests, which enables multiple parties to collaboratively train a machine learning model without exchanging their local data. A key and common challenge on distributed databases is the heterogeneity of the data distribution among the parties. The data of different parties are usually non-independently and identically distributed (i.e., non-IID). There have been many FL algorithms to address the learning effectiveness under non-IID data settings. However, there lacks an experimental study on systematically understanding their advantages and disadvantages, as previous studies have very rigid data partitioning strategies among parties, which are hardly representative and thorough. In this paper, to help researchers better understand and study the non-IID data setting in federated learning, we propose comprehensive data partitioning strategies to cover the typical non-IID data cases. Moreover, we conduct extensive experiments to evaluate state-of-the-art FL algorithms. We find that non-IID does bring significant challenges in learning accuracy of FL algorithms, and none of the existing state-of-the-art FL algorithms outperforms others in all cases. Our experiments provide insights for future studies of addressing the challenges in "data silos".},
  archiveprefix = {arXiv},
  file = {/home/javier/Zotero/storage/IQ9PYUR9/Li et al. - 2021 - Federated Learning on Non-IID Data Silos An Exper.pdf;/home/javier/Zotero/storage/UKIJ3SD9/2102.html}
}

@article{long,
  title = {G-{{PATE}}: {{Scalable Differentially Private Data Generator}} via {{Private Aggregation}} of {{Teacher Discriminators}}},
  author = {Long, Yunhui and Wang, Boxin and Yang, Zhuolin and Kailkhura, Bhavya and Zhang, Aston and Gunter, Carl A and Li, Bo},
  abstract = {Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator which preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator. Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ("  1). Our code is available at https://github.com/AI-secure/G-PATE.},
  langid = {english},
  file = {/home/javier/Zotero/storage/7YNGZ28G/Long et al. - G-PATE Scalable Differentially Private Data Gener.pdf}
}

@misc{mcmahan2023a,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2023},
  month = jan,
  number = {arXiv:1602.05629},
  eprint = {1602.05629},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arXiv},
  file = {/home/javier/Zotero/storage/EBNMC2MY/McMahan et al. - 2023 - Communication-Efficient Learning of Deep Networks .pdf;/home/javier/Zotero/storage/3WPBK7V8/1602.html}
}

@book{murphy2022,
  title = {Probabilistic Machine Learning: An Introduction},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2022},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR".--},
  isbn = {978-0-262-36930-5 978-0-262-04682-4},
  langid = {english},
  file = {/home/javier/Zotero/storage/6K22P9BR/Murphy - 2022 - Probabilistic machine learning an introduction.pdf}
}

@misc{oshea2015,
  title = {An {{Introduction}} to {{Convolutional Neural Networks}}},
  author = {O'Shea, Keiron and Nash, Ryan},
  year = {2015},
  month = dec,
  number = {arXiv:1511.08458},
  eprint = {1511.08458},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-26},
  abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/GN92FWJP/O'Shea and Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf}
}

@misc{papernot2017,
  title = {Semi-Supervised {{Knowledge Transfer}} for {{Deep Learning}} from {{Private Training Data}}},
  author = {Papernot, Nicolas and Abadi, Mart{\'i}n and Erlingsson, {\'U}lfar and Goodfellow, Ian and Talwar, Kunal},
  year = {2017},
  month = mar,
  number = {arXiv:1610.05755},
  eprint = {1610.05755},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-18},
  abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/K43LB78Z/Papernot et al. - 2017 - Semi-supervised Knowledge Transfer for Deep Learni.pdf}
}

@article{qian1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  year = {1999},
  month = jan,
  journal = {Neural Networks: The Official Journal of the International Neural Network Society},
  volume = {12},
  number = {1},
  pages = {145--151},
  issn = {1879-2782},
  doi = {10.1016/s0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  langid = {english},
  pmid = {12662723}
}

@misc{reddi2021,
  title = {Adaptive {{Federated Optimization}}},
  author = {Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v c}n{\'y}, Jakub and Kumar, Sanjiv and McMahan, H. Brendan},
  year = {2021},
  month = sep,
  number = {arXiv:2003.00295},
  eprint = {2003.00295},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FEDAVG) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including ADAGRAD, ADAM, and YOGI, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/6CRWTTBL/Reddi et al. - 2021 - Adaptive Federated Optimization.pdf}
}

@article{rosenblatt1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2024-05-22},
  abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
  langid = {english}
}

@misc{ruder2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  number = {arXiv:1609.04747},
  eprint = {1609.04747},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/EL94DSBJ/Ruder - 2017 - An overview of gradient descent optimization algor.pdf}
}

@article{rumelhart1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2024-05-26},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english}
}

@techreport{sahu2017,
  title = {Review {{Paper}} on {{Secure Hash Algorithm With Its Variants}}},
  author = {Sahu, Aradhana and Ghosh, Samarendra},
  year = {2017},
  month = may,
  doi = {10.13140/RG.2.2.13855.05289},
  file = {/home/javier/Zotero/storage/KJE4ZSD3/Sahu and Ghosh - 2017 - Review Paper on Secure Hash Algorithm With Its Var.pdf}
}

@misc{shokri2017,
  title = {Membership {{Inference Attacks}} against {{Machine Learning Models}}},
  author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  year = {2017},
  month = mar,
  number = {arXiv:1610.05820},
  eprint = {1610.05820},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-20},
  abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/4535AX8C/1610.pdf}
}

@inproceedings{smestad2023,
  title = {A {{Systematic Literature Review}} on {{Client Selection}} in {{Federated Learning}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  author = {Smestad, Carl and Li, Jingyue},
  year = {2023},
  month = jun,
  eprint = {2306.04862},
  primaryclass = {cs},
  pages = {2--11},
  doi = {10.1145/3593434.3593438},
  urldate = {2024-05-11},
  abstract = {With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rounds, as testing accuracy measures the successfulness of the learning and preferably in as few communication rounds as possible, as they are very expensive. Although several possible improvements can be made with the current state of client selection, the most beneficial ones are evaluating the impact of unsuccessful clients and gaining a more theoretical understanding of the impact of fairness in FL.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/3WCT4EXX/Smestad and Li - 2023 - A Systematic Literature Review on Client Selection.pdf}
}

@misc{sun2019,
  title = {A {{Survey}} of {{Optimization Methods}} from a {{Machine Learning Perspective}}},
  author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  year = {2019},
  month = oct,
  number = {arXiv:1906.06821},
  eprint = {1906.06821},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/UJ6PH7EH/Sun et al. - 2019 - A Survey of Optimization Methods from a Machine Le.pdf}
}

@misc{sun2019a,
  title = {Optimization for Deep Learning: Theory and Algorithms},
  shorttitle = {Optimization for Deep Learning},
  author = {Sun, Ruoyu},
  year = {2019},
  month = dec,
  number = {arXiv:1912.08957},
  eprint = {1912.08957},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and existing theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinitewidth analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/CTWKWQCM/Sun - 2019 - Optimization for deep learning theory and algorit.pdf}
}

@article{sun2023,
  title = {Adversarial {{Attacks Against Deep Generative Models}} on {{Data}}: {{A Survey}}},
  shorttitle = {Adversarial {{Attacks Against Deep Generative Models}} on {{Data}}},
  author = {Sun, Hui and Zhu, Tianqing and Zhang, Zhiqiu and Jin, Dawei and Xiong, Ping and Zhou, Wanlei},
  year = {2023},
  month = apr,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {4},
  pages = {3367--3388},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2021.3130903},
  urldate = {2024-07-14},
  abstract = {Deep generative models have gained much attention given their ability to generate data for applications as varied as healthcare to financial technology to surveillance, and many more - the most popular models being generative adversarial networks (GANs) and variational auto-encoders (VAEs). Yet, as with all machine learning models, ever is the concern over security breaches and privacy leaks and deep generative models are no exception. In fact, these models have advanced so rapidly in recent years that work on their security is still in its infancy. In an attempt to audit the current and future threats against these models, and to provide a roadmap for defense preparations in the short term, we prepared this comprehensive and specialized survey on the security and privacy preservation of GANs and VAEs. Our focus is on the inner connection between attacks and model architectures and, more specifically, on five components of deep generative models: the training data, the latent code, the generators/decoders of GANs/VAEs, the discriminators/encoders of GANs/VAEs, and the generated data. For each model, component and attack, we review the current research progress and identify the key challenges. The paper concludes with a discussion of possible future attacks and research directions in the field.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/home/javier/Zotero/storage/J2ANEYZQ/Sun et al. - 2023 - Adversarial Attacks Against Deep Generative Models.pdf}
}

@misc{wagh2018,
  title = {{{SecureNN}}: {{Efficient}} and {{Private Neural Network Training}}},
  shorttitle = {{{SecureNN}}},
  author = {Wagh, Sameer and Gupta, Divya and Chandran, Nishanth},
  year = {2018},
  number = {2018/442},
  urldate = {2024-07-01},
  abstract = {Neural Networks (NN) provide a powerful method for machine learning training and inference. To effectively train, it is desirable for multiple parties to combine their data -- however, doing so conflicts with data privacy. In this work, we provide novel three-party secure computation protocols for various NN building blocks such as matrix multiplication, convolutions, Rectified Linear Units, Maxpool, normalization and so on. This enables us to construct three-party secure protocols for training and inference of several NN architectures such that no single party learns any information about the data. Experimentally, we implement our system over Amazon EC2 servers in different settings. {\textbackslash}{\textbackslash} Our work advances the state-of-the-art of secure computation for neural networks in three ways: {\textbackslash}begin\{enumerate\} {\textbackslash}item Scalability: We are the first work to provide neural network training on Convolutional Neural Networks (CNNs) that have an accuracy of \${$>$}99{\textbackslash}\%\$ on the MNIST dataset; {\textbackslash}item Performance: For secure inference, our system outperforms prior 2 and 3-server works (SecureML, MiniONN, Chameleon, Gazelle) by \$6{\textbackslash}times\$-\$113{\textbackslash}times\$ (with larger gains obtained in more complex networks). Our total execution times are \$2-4{\textbackslash}times\$ faster than even just the online times of these works. For secure training, compared to the only prior work (SecureML) that considered a much smaller fully connected network, our protocols are \$79{\textbackslash}times\$ and \$7{\textbackslash}times\$ faster than their 2 and 3-server protocols. In the WAN setting, these improvements are more dramatic and we obtain an improvement of \$553{\textbackslash}times\$! {\textbackslash}item Security: Our protocols provide two kinds of security: full security (privacy and correctness) against one semi-honest corruption and the notion of privacy against one malicious corruption [Araki{\textasciitilde}{\textbackslash}etal{\textasciitilde}CCS'16]. All prior works only provide semi-honest security and ours is the first system to provide any security against malicious adversaries for the secure computation of complex algorithms such as neural network inference and training. {\textbackslash}end\{enumerate\} Our gains come from a significant improvement in communication through the elimination of expensive garbled circuits and oblivious transfer protocols.},
  annotation = {Publication info: Published elsewhere. 19th Privacy Enhancing Technologies Symposium (PETS 2019)},
  file = {/home/javier/Zotero/storage/A5X7WWN4/Wagh et al. - 2018 - SecureNN Efficient and Private Neural Network Tra.pdf}
}

@misc{wang2020,
  title = {Tackling the {{Objective Inconsistency Problem}} in {{Heterogeneous Federated Optimization}}},
  author = {Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H. Vincent},
  year = {2020},
  month = jul,
  number = {arXiv:2007.07481},
  eprint = {2007.07481},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-15},
  abstract = {In federated optimization, heterogeneity in the clients' local datasets and computation speeds results in large variations in the number of local updates performed by each client in each communication round. Naive weighted aggregation of such models causes objective inconsistency, that is, the global model converges to a stationary point of a mismatched objective function which can be arbitrarily different from the true objective. This paper provides a general framework to analyze the convergence of federated heterogeneous optimization algorithms. It subsumes previously proposed methods such as FedAvg and FedProx and provides the first principled understanding of the solution bias and the convergence slowdown due to objective inconsistency. Using insights from this analysis, we propose FedNova, a normalized averaging method that eliminates objective inconsistency while preserving fast error convergence.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/6BJW2WEK/Wang et al. - 2020 - Tackling the Objective Inconsistency Problem in He.pdf}
}

@misc{wang2021,
  title = {A {{Field Guide}} to {{Federated Optimization}}},
  author = {Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H. Brendan and y Arcas, Blaise Aguera and {Al-Shedivat}, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and Diggavi, Suhas and Eichner, Hubert and Gadhikar, Advait and Garrett, Zachary and Girgis, Antonious M. and Hanzely, Filip and Hard, Andrew and He, Chaoyang and Horvath, Samuel and Huo, Zhouyuan and Ingerman, Alex and Jaggi, Martin and Javidi, Tara and Kairouz, Peter and Kale, Satyen and Karimireddy, Sai Praneeth and Konecny, Jakub and Koyejo, Sanmi and Li, Tian and Liu, Luyang and Mohri, Mehryar and Qi, Hang and Reddi, Sashank J. and Richtarik, Peter and Singhal, Karan and Smith, Virginia and Soltanolkotabi, Mahdi and Song, Weikang and Suresh, Ananda Theertha and Stich, Sebastian U. and Talwalkar, Ameet and Wang, Hongyi and Woodworth, Blake and Wu, Shanshan and Yu, Felix X. and Yuan, Honglin and Zaheer, Manzil and Zhang, Mi and Zhang, Tong and Zheng, Chunxiang and Zhu, Chen and Zhu, Wennan},
  year = {2021},
  month = jul,
  number = {arXiv:2107.06917},
  eprint = {2107.06917},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-14},
  abstract = {Federated learning and analytics are a distributed approach for collaboratively learning models (or statistics) from decentralized data, motivated by and designed for privacy protection. The distributed learning process can be formulated as solving federated optimization problems, which emphasize communication efficiency, data heterogeneity, compatibility with privacy and system requirements, and other constraints that are not primary considerations in other problem settings. This paper provides recommendations and guidelines on formulating, designing, evaluating and analyzing federated optimization algorithms through concrete examples and practical implementation, with a focus on conducting effective simulations to infer real-world performance. The goal of this work is not to survey the current literature, but to inspire researchers and practitioners to design federated learning algorithms that can be used in various practical applications.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/JBBWY97N/Wang et al. - 2021 - A Field Guide to Federated Optimization.pdf}
}

@article{wood2018,
  title = {Differential {{Privacy}}: {{A Primer}} for a {{Non-Technical Audience}}},
  shorttitle = {Differential {{Privacy}}},
  author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
  year = {2018},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3338027},
  urldate = {2024-07-20},
  langid = {english},
  file = {/home/javier/Zotero/storage/7Z5BJLU2/Wood et al. - 2018 - Differential Privacy A Primer for a Non-Technical.pdf}
}

@incollection{xu2019,
  title = {Modeling Tabular Data Using Conditional {{GAN}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Xu, Lei and Skoularidou, Maria and {Cuesta-Infante}, Alfredo and Veeramachaneni, Kalyan},
  year = {2019},
  month = dec,
  number = {659},
  pages = {7335--7345},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-07-13},
  abstract = {Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design CTGAN, which uses a conditional generator to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. CTGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.}
}

@article{y1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}}(1/K**2)},
  author = {Y, Nesterov},
  year = {1983},
  journal = {Doklady Akademii Nauk SSSR},
  volume = {269},
  number = {3},
  pages = {543},
  urldate = {2024-05-25},
  file = {/home/javier/Zotero/storage/PHFGDURC/1370017280653524239.html}
}

@article{yang2022,
  title = {Federated {{Learning}} with {{Nesterov Accelerated Gradient}}},
  author = {Yang, Zhengjie and Bao, Wei and Yuan, Dong and Tran, Nguyen H. and Zomaya, Albert Y.},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {12},
  eprint = {2009.08716},
  primaryclass = {cs, stat},
  pages = {4863--4873},
  issn = {1045-9219, 1558-2183, 2161-9883},
  doi = {10.1109/TPDS.2022.3206480},
  urldate = {2024-07-23},
  abstract = {Federated learning (FL) is a fast-developing technique that allows multiple workers to train a global model based on a distributed dataset. Conventional FL (FedAvg) employs gradient descent algorithm, which may not be efficient enough. Momentum is able to improve the situation by adding an additional momentum step to accelerate the convergence and has demonstrated its benefits in both centralized and FL environments. It is well-known that Nesterov Accelerated Gradient (NAG) is a more advantageous form of momentum, but it is not clear how to quantify the benefits of NAG in FL so far. This motives us to propose FedNAG, which employs NAG in each worker as well as NAG momentum and model aggregation in the aggregator. We provide a detailed convergence analysis of FedNAG and compare it with FedAvg. Extensive experiments based on real-world datasets and trace-driven simulation are conducted, demonstrating that FedNAG increases the learning accuracy by 3--24\% and decreases the total training time by 11--70\% compared with the benchmarks under a wide range of settings.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/F792TP6V/Yang et al. - 2022 - Federated Learning with Nesterov Accelerated Gradi.pdf}
}

@misc{yousefpour2022,
  title = {Opacus: {{User-Friendly Differential Privacy Library}} in {{PyTorch}}},
  shorttitle = {Opacus},
  author = {Yousefpour, Ashkan and Shilov, Igor and Sablayrolles, Alexandre and Testuggine, Davide and Prasad, Karthik and Malek, Mani and Nguyen, John and Ghosh, Sayan and Bharadwaj, Akash and Zhao, Jessica and Cormode, Graham and Mironov, Ilya},
  year = {2022},
  month = aug,
  number = {arXiv:2109.12298},
  eprint = {2109.12298},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-21},
  abstract = {We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional ``micro batch'' approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/ZMZ7796A/Yousefpour et al. - 2022 - Opacus User-Friendly Differential Privacy Library.pdf}
}

@misc{zeiler2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  year = {2012},
  month = dec,
  number = {arXiv:1212.5701},
  eprint = {1212.5701},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-26},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/TPZSZLST/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf}
}

@misc{zhang2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  author = {Zhang, Zhilu and Sabuncu, Mert R.},
  year = {2018},
  month = nov,
  number = {arXiv:1805.07836},
  eprint = {1805.07836},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHIONMNIST datasets and synthetically generated noisy labels.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/GIXJLZB3/Zhang and Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep N.pdf}
}

@misc{zhang2023,
  title = {Private {{Federated Learning}} in {{Gboard}}},
  author = {Zhang, Yuanbo and Ramage, Daniel and Xu, Zheng and Zhang, Yanxiang and Zhai, Shumin and Kairouz, Peter},
  year = {2023},
  month = jun,
  number = {arXiv:2306.14793},
  eprint = {2306.14793},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-11},
  abstract = {This white paper describes recent advances in Gboard(Google Keyboard)'s use of federated learning, DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm, and secure aggregation techniques to train machine learning (ML) models for suggestion, prediction and correction intelligence from many users' typing data. Gboard's investment in those privacy technologies allows users' typing data to be processed locally on device, to be aggregated as early as possible, and to have strong anonymization and differential privacy where possible. Technical strategies and practices have been established to allow ML models to be trained and deployed with meaningfully formal DP guarantees and high utility. The paper also looks ahead to how technologies such as trusted execution environments may be used to further improve the privacy and security of Gboard's ML models.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/ZEU4USGI/Zhang et al. - 2023 - Private Federated Learning in Gboard.pdf}
}

@article{zhao2018,
  title = {Federated {{Learning}} with {{Non-IID Data}}},
  author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  year = {2018},
  eprint = {1806.00582},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1806.00582},
  urldate = {2024-01-12},
  abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55\% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30\% for the CIFAR-10 dataset with only 5\% globally shared data.},
  archiveprefix = {arXiv},
  file = {/home/javier/Zotero/storage/7T3IPGCR/Zhao et al. - 2018 - Federated Learning with Non-IID Data.pdf;/home/javier/Zotero/storage/EMHTM8BJ/1806.html}
}

@misc{zotero-166,
  title = {Data {{Act}} {\textbar} {{Shaping Europe}}'s Digital Future},
  urldate = {2024-07-27},
  howpublished = {https://digital-strategy.ec.europa.eu/en/policies/data-act},
  langid = {english},
  file = {/home/javier/Zotero/storage/2BCI7IJQ/data-act.html}
}

@misc{zotero-168,
  title = {As{\'i} Es La Nueva {{Ordenanza Tipo}} de La {{FEMP}} Sobre {{Gobierno}} Del {{Dato}} {\textbar} Datos.Gob.Es},
  urldate = {2024-07-27},
  howpublished = {https://datos.gob.es/es/blog/asi-es-la-nueva-ordenanza-tipo-de-la-femp-sobre-gobierno-del-dato},
  file = {/home/javier/Zotero/storage/D6PYRZYP/asi-es-la-nueva-ordenanza-tipo-de-la-femp-sobre-gobierno-del-dato.html}
}
