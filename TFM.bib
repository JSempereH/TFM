@book{calin2020,
  title = {Deep {{Learning Architectures}}: {{A Mathematical Approach}}},
  shorttitle = {Deep {{Learning Architectures}}},
  author = {Calin, Ovidiu},
  year = {2020},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-36721-3},
  urldate = {2024-05-23},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-3-030-36720-6 978-3-030-36721-3},
  langid = {english},
  file = {/home/javier/Zotero/storage/3RFUWDFU/Calin - 2020 - Deep Learning Architectures A Mathematical Approa.pdf}
}

@misc{dauphin2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2572},
  eprint = {1406.2572},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/7QCEKUB7/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf}
}

@inproceedings{glorot2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2024-05-23},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  langid = {english},
  file = {/home/javier/Zotero/storage/VSVF7YY6/Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf}
}

@book{goodfellow2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016}
}

@misc{kairouz2021,
  title = {Advances and {{Open Problems}} in {{Federated Learning}}},
  author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gasc{\'o}n, Adri{\`a} and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Kone{\v c}n{\'y}, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancr{\`e}de and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and {\"O}zg{\"u}r, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tram{\`e}r, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
  year = {2021},
  month = mar,
  number = {arXiv:1912.04977},
  eprint = {1912.04977},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-15},
  abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/SYCUPYBQ/Kairouz et al. - 2021 - Advances and Open Problems in Federated Learning.pdf}
}

@article{krizhevsky2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2024-05-23},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/home/javier/Zotero/storage/IQRNZVQT/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@misc{li2020,
  title = {Federated {{Optimization}} in {{Heterogeneous Networks}}},
  author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  year = {2020},
  month = apr,
  number = {arXiv:1812.06127},
  eprint = {1812.06127},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-16},
  abstract = {Federated Learning is a distributed learning paradigm with two key challenges that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systems characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). In this work, we introduce a framework, FedProx, to tackle heterogeneity in federated networks. FedProx can be viewed as a generalization and re-parametrization of FedAvg, the current state-of-the-art method for federated learning. While this re-parameterization makes only minor modifications to the method itself, these modifications have important ramifications both in theory and in practice. Theoretically, we provide convergence guarantees for our framework when learning over data from non-identical distributions (statistical heterogeneity), and while adhering to device-level systems constraints by allowing each participating device to perform a variable amount of work (systems heterogeneity). Practically, we demonstrate that FedProx allows for more robust convergence than FedAvg across a suite of realistic federated datasets. In particular, in highly heterogeneous settings, FedProx demonstrates significantly more stable and accurate convergence behavior relative to FedAvg---improving absolute test accuracy by 22\% on average.},
  archiveprefix = {arxiv},
  file = {/home/javier/Zotero/storage/9D2E5TRQ/Li et al. - 2020 - Federated Optimization in Heterogeneous Networks.pdf;/home/javier/Zotero/storage/DGCZVR7F/1812.html}
}

@misc{li2021,
  title = {Federated {{Learning}} on {{Non-IID Data Silos}}: {{An Experimental Study}}},
  shorttitle = {Federated {{Learning}} on {{Non-IID Data Silos}}},
  author = {Li, Qinbin and Diao, Yiqun and Chen, Quan and He, Bingsheng},
  year = {2021},
  month = oct,
  number = {arXiv:2102.02079},
  eprint = {2102.02079},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Due to the increasing privacy concerns and data regulations, training data have been increasingly fragmented, forming distributed databases of multiple "data silos" (e.g., within different organizations and countries). To develop effective machine learning services, there is a must to exploit data from such distributed databases without exchanging the raw data. Recently, federated learning (FL) has been a solution with growing interests, which enables multiple parties to collaboratively train a machine learning model without exchanging their local data. A key and common challenge on distributed databases is the heterogeneity of the data distribution among the parties. The data of different parties are usually non-independently and identically distributed (i.e., non-IID). There have been many FL algorithms to address the learning effectiveness under non-IID data settings. However, there lacks an experimental study on systematically understanding their advantages and disadvantages, as previous studies have very rigid data partitioning strategies among parties, which are hardly representative and thorough. In this paper, to help researchers better understand and study the non-IID data setting in federated learning, we propose comprehensive data partitioning strategies to cover the typical non-IID data cases. Moreover, we conduct extensive experiments to evaluate state-of-the-art FL algorithms. We find that non-IID does bring significant challenges in learning accuracy of FL algorithms, and none of the existing state-of-the-art FL algorithms outperforms others in all cases. Our experiments provide insights for future studies of addressing the challenges in "data silos".},
  archiveprefix = {arxiv},
  file = {/home/javier/Zotero/storage/IQ9PYUR9/Li et al. - 2021 - Federated Learning on Non-IID Data Silos An Exper.pdf;/home/javier/Zotero/storage/UKIJ3SD9/2102.html}
}

@misc{mcmahan2023a,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2023},
  month = jan,
  number = {arXiv:1602.05629},
  eprint = {1602.05629},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arxiv},
  file = {/home/javier/Zotero/storage/EBNMC2MY/McMahan et al. - 2023 - Communication-Efficient Learning of Deep Networks .pdf;/home/javier/Zotero/storage/3WPBK7V8/1602.html}
}

@book{murphy2022,
  title = {Probabilistic Machine Learning: An Introduction},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2022},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR".--},
  isbn = {978-0-262-36930-5 978-0-262-04682-4},
  langid = {english},
  file = {/home/javier/Zotero/storage/6K22P9BR/Murphy - 2022 - Probabilistic machine learning an introduction.pdf}
}

@article{qian1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  year = {1999},
  month = jan,
  journal = {Neural Networks: The Official Journal of the International Neural Network Society},
  volume = {12},
  number = {1},
  pages = {145--151},
  issn = {1879-2782},
  doi = {10.1016/s0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  langid = {english},
  pmid = {12662723}
}

@article{rosenblatt1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2024-05-22},
  abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
  langid = {english}
}

@misc{ruder2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  number = {arXiv:1609.04747},
  eprint = {1609.04747},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/EL94DSBJ/Ruder - 2017 - An overview of gradient descent optimization algor.pdf}
}

@inproceedings{smestad2023,
  title = {A {{Systematic Literature Review}} on {{Client Selection}} in {{Federated Learning}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  author = {Smestad, Carl and Li, Jingyue},
  year = {2023},
  month = jun,
  eprint = {2306.04862},
  primaryclass = {cs},
  pages = {2--11},
  doi = {10.1145/3593434.3593438},
  urldate = {2024-05-11},
  abstract = {With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rounds, as testing accuracy measures the successfulness of the learning and preferably in as few communication rounds as possible, as they are very expensive. Although several possible improvements can be made with the current state of client selection, the most beneficial ones are evaluating the impact of unsuccessful clients and gaining a more theoretical understanding of the impact of fairness in FL.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/3WCT4EXX/Smestad and Li - 2023 - A Systematic Literature Review on Client Selection.pdf}
}

@misc{sun2019,
  title = {A {{Survey}} of {{Optimization Methods}} from a {{Machine Learning Perspective}}},
  author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  year = {2019},
  month = oct,
  number = {arXiv:1906.06821},
  eprint = {1906.06821},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/UJ6PH7EH/Sun et al. - 2019 - A Survey of Optimization Methods from a Machine Le.pdf}
}

@misc{sun2019a,
  title = {Optimization for Deep Learning: Theory and Algorithms},
  shorttitle = {Optimization for Deep Learning},
  author = {Sun, Ruoyu},
  year = {2019},
  month = dec,
  number = {arXiv:1912.08957},
  eprint = {1912.08957},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and existing theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinitewidth analysis.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/CTWKWQCM/Sun - 2019 - Optimization for deep learning theory and algorit.pdf}
}

@misc{wang2020,
  title = {Tackling the {{Objective Inconsistency Problem}} in {{Heterogeneous Federated Optimization}}},
  author = {Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H. Vincent},
  year = {2020},
  month = jul,
  number = {arXiv:2007.07481},
  eprint = {2007.07481},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-15},
  abstract = {In federated optimization, heterogeneity in the clients' local datasets and computation speeds results in large variations in the number of local updates performed by each client in each communication round. Naive weighted aggregation of such models causes objective inconsistency, that is, the global model converges to a stationary point of a mismatched objective function which can be arbitrarily different from the true objective. This paper provides a general framework to analyze the convergence of federated heterogeneous optimization algorithms. It subsumes previously proposed methods such as FedAvg and FedProx and provides the first principled understanding of the solution bias and the convergence slowdown due to objective inconsistency. Using insights from this analysis, we propose FedNova, a normalized averaging method that eliminates objective inconsistency while preserving fast error convergence.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/6BJW2WEK/Wang et al. - 2020 - Tackling the Objective Inconsistency Problem in He.pdf}
}

@misc{wang2021,
  title = {A {{Field Guide}} to {{Federated Optimization}}},
  author = {Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H. Brendan and y Arcas, Blaise Aguera and {Al-Shedivat}, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and Diggavi, Suhas and Eichner, Hubert and Gadhikar, Advait and Garrett, Zachary and Girgis, Antonious M. and Hanzely, Filip and Hard, Andrew and He, Chaoyang and Horvath, Samuel and Huo, Zhouyuan and Ingerman, Alex and Jaggi, Martin and Javidi, Tara and Kairouz, Peter and Kale, Satyen and Karimireddy, Sai Praneeth and Konecny, Jakub and Koyejo, Sanmi and Li, Tian and Liu, Luyang and Mohri, Mehryar and Qi, Hang and Reddi, Sashank J. and Richtarik, Peter and Singhal, Karan and Smith, Virginia and Soltanolkotabi, Mahdi and Song, Weikang and Suresh, Ananda Theertha and Stich, Sebastian U. and Talwalkar, Ameet and Wang, Hongyi and Woodworth, Blake and Wu, Shanshan and Yu, Felix X. and Yuan, Honglin and Zaheer, Manzil and Zhang, Mi and Zhang, Tong and Zheng, Chunxiang and Zhu, Chen and Zhu, Wennan},
  year = {2021},
  month = jul,
  number = {arXiv:2107.06917},
  eprint = {2107.06917},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-14},
  abstract = {Federated learning and analytics are a distributed approach for collaboratively learning models (or statistics) from decentralized data, motivated by and designed for privacy protection. The distributed learning process can be formulated as solving federated optimization problems, which emphasize communication efficiency, data heterogeneity, compatibility with privacy and system requirements, and other constraints that are not primary considerations in other problem settings. This paper provides recommendations and guidelines on formulating, designing, evaluating and analyzing federated optimization algorithms through concrete examples and practical implementation, with a focus on conducting effective simulations to infer real-world performance. The goal of this work is not to survey the current literature, but to inspire researchers and practitioners to design federated learning algorithms that can be used in various practical applications.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/JBBWY97N/Wang et al. - 2021 - A Field Guide to Federated Optimization.pdf}
}

@article{y1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}}(1/K**2)},
  author = {Y, Nesterov},
  year = {1983},
  journal = {Doklady Akademii Nauk SSSR},
  volume = {269},
  number = {3},
  pages = {543},
  urldate = {2024-05-25},
  file = {/home/javier/Zotero/storage/PHFGDURC/1370017280653524239.html}
}

@misc{zhang2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  author = {Zhang, Zhilu and Sabuncu, Mert R.},
  year = {2018},
  month = nov,
  number = {arXiv:1805.07836},
  eprint = {1805.07836},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-24},
  abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHIONMNIST datasets and synthetically generated noisy labels.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/GIXJLZB3/Zhang and Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep N.pdf}
}

@misc{zhang2023,
  title = {Private {{Federated Learning}} in {{Gboard}}},
  author = {Zhang, Yuanbo and Ramage, Daniel and Xu, Zheng and Zhang, Yanxiang and Zhai, Shumin and Kairouz, Peter},
  year = {2023},
  month = jun,
  number = {arXiv:2306.14793},
  eprint = {2306.14793},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-11},
  abstract = {This white paper describes recent advances in Gboard(Google Keyboard)'s use of federated learning, DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm, and secure aggregation techniques to train machine learning (ML) models for suggestion, prediction and correction intelligence from many users' typing data. Gboard's investment in those privacy technologies allows users' typing data to be processed locally on device, to be aggregated as early as possible, and to have strong anonymization and differential privacy where possible. Technical strategies and practices have been established to allow ML models to be trained and deployed with meaningfully formal DP guarantees and high utility. The paper also looks ahead to how technologies such as trusted execution environments may be used to further improve the privacy and security of Gboard's ML models.},
  archiveprefix = {arxiv},
  langid = {english},
  file = {/home/javier/Zotero/storage/ZEU4USGI/Zhang et al. - 2023 - Private Federated Learning in Gboard.pdf}
}

@article{zhao2018,
  title = {Federated {{Learning}} with {{Non-IID Data}}},
  author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  year = {2018},
  eprint = {1806.00582},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1806.00582},
  urldate = {2024-01-12},
  abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55\% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30\% for the CIFAR-10 dataset with only 5\% globally shared data.},
  archiveprefix = {arxiv},
  file = {/home/javier/Zotero/storage/7T3IPGCR/Zhao et al. - 2018 - Federated Learning with Non-IID Data.pdf;/home/javier/Zotero/storage/EMHTM8BJ/1806.html}
}
