{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Es el nodo\n",
    "class SplitNNServicer:\n",
    "    def __init__(self, dataset_train, dataset_test):\n",
    "        self.dataset = torch.utils.data.DataLoader(dataset_train, \n",
    "                                                   batch_size=64, \n",
    "                                                   shuffle=False)\n",
    "        self.dataset_test = dataset_test\n",
    "        self.all_servicers: List[SplitNNServicer] = []\n",
    "        self.database = {}\n",
    "    \n",
    "    def FirstForward(self, images) -> torch.Tensor:\n",
    "        out = self.database[\"model\"](images)\n",
    "        return out\n",
    "        #return self.all_servicers[1].SecondForward(out)\n",
    "        # es legal hacer self.all_servicers[1].Forward(...)\n",
    "    \n",
    "    def SecondForward(self, in_: torch.Tensor) -> torch.Tensor:\n",
    "        return self.database[\"model\"](in_)\n",
    "    \n",
    "    def SubirModelo(self, modelo, optimizador):\n",
    "        self.database[\"model\"] = modelo\n",
    "        self.database[\"optimizer\"] = optimizador\n",
    "\n",
    "    def MetodoRandom(self, tensor_input: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor_input + self.global_dict[\"hola\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:03<00:00, 2793306.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpf1r9p53y/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 316366.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpf1r9p53y/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2745232.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpf1r9p53y/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2016142.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpf1r9p53y/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpf1r9p53y/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_dataset = torchvision.datasets.MNIST(temp_dir, train=True, download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            (0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(temp_dir, train=False, download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            (0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "class AsdfDataset(Dataset):\n",
    "    def __init__(self, dataset, index):\n",
    "        self.dataset = dataset\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        true_sample = self.dataset[idx]\n",
    "        return true_sample[self.index]\n",
    "\n",
    "\n",
    "features_dataset = AsdfDataset(full_dataset, 0)\n",
    "labels_dataset = AsdfDataset(full_dataset, 1)\n",
    "\n",
    "features_dataset_test = AsdfDataset(test_dataset, 0)\n",
    "labels_dataset_test   = AsdfDataset(test_dataset, 1)\n",
    "\n",
    "alice = SplitNNServicer(features_dataset,features_dataset_test)\n",
    "bob = SplitNNServicer(labels_dataset, labels_dataset_test)\n",
    "\n",
    "alice.all_servicers = [alice, bob]\n",
    "bob.all_servicers = [alice, bob]\n",
    "\n",
    "# A partir de ahora, solo podemos acceder a alice.MetodoEnMayusculas y bob.LoMismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "### client.py\n",
    "class SplitNN_v1:\n",
    "    def __init__(self, \n",
    "                    node_features: SplitNNServicer, model_features, optimizer_features,\n",
    "                    node_labels: SplitNNServicer, model_labels, optimizer_labels,\n",
    "    ):\n",
    "        self.node_features = node_features\n",
    "        self.model_features = model_features\n",
    "        self.optimizer_features = optimizer_features\n",
    "        \n",
    "        self.node_features.SubirModelo(self.model_features, self.optimizer_features)\n",
    "        \n",
    "        \n",
    "        self.node_labels = node_labels\n",
    "        self.model_labels = model_labels\n",
    "        self.optimizer_labels = optimizer_labels\n",
    "        \n",
    "        self.node_labels.SubirModelo(self.model_labels, self.optimizer_labels)\n",
    "\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        loss_per_epoch = []\n",
    "        accuracy_per_epoch = []\n",
    "\n",
    "        test_accuracy_per_epoch = []\n",
    "        \n",
    "        for i in tqdm(range(1,n_epochs+1), desc = \"Epochs\"):\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in zip(self.node_features.dataset, self.node_labels.dataset):\n",
    "                images = images.view(images.shape[0],-1)\n",
    "                \n",
    "                # 1) Zero our grads\n",
    "                self.node_features.database['optimizer'].zero_grad()\n",
    "                self.node_labels.database['optimizer'].zero_grad()\n",
    "            \n",
    "                # 2) Make a prediction\n",
    "                a = []\n",
    "                remote_a = []\n",
    "                a.append(self.node_features.FirstForward(images))\n",
    "                remote_a.append(a[-1].detach().requires_grad_())\n",
    "                a.append(self.node_labels.SecondForward(remote_a[-1]))\n",
    "                remote_a.append(a[-1].detach().requires_grad_())\n",
    "                \n",
    "                \n",
    "                # 3) Figure out how much we missed by\n",
    "                criterion = nn.NLLLoss()\n",
    "                loss = criterion(a[-1], labels)\n",
    "                \n",
    "                #4) Backprop the loss on the end sub_model\n",
    "                loss.backward()\n",
    "                \n",
    "                #5) Feed Gradients backward through the network\n",
    "                grad_a = remote_a[0].grad\n",
    "                a[0].backward(grad_a)\n",
    "                \n",
    "                #6) Change the weights\n",
    "                self.node_features.database['optimizer'].step()\n",
    "                self.node_labels.database['optimizer'].step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(a[-1].data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            \n",
    "            loss_per_epoch.append(running_loss/len(self.node_features.dataset))\n",
    "            # print(f\"Epoch {i} - Training loss: {running_loss/len(self.node_features.dataset)}\")\n",
    "            accuracy_per_epoch.append(100*correct/total)\n",
    "\n",
    "            current_test_accuracy = self.test()\n",
    "            test_accuracy_per_epoch.append(current_test_accuracy)\n",
    "\n",
    "\n",
    "        \n",
    "        # Save results to CSV\n",
    "        with open(\"Train_metrics.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\"])\n",
    "            for epoch in range(n_epochs):\n",
    "                writer.writerow([epoch + 1, loss_per_epoch[epoch], accuracy_per_epoch[epoch]])\n",
    "        \n",
    "        with open(\"Test_metrics.csv\", mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Epoch\", \"Test Accuracy\"])\n",
    "            for epoch in range(n_epochs):\n",
    "                writer.writerow([epoch + 1, test_accuracy_per_epoch[epoch]])\n",
    "\n",
    "                \n",
    "                \n",
    "    \n",
    "    def test(self):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in zip(self.node_features.dataset_test, self.node_labels.dataset_test):\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            a = []\n",
    "            remote_a = []\n",
    "            a.append(self.node_features.FirstForward(images))\n",
    "            remote_a.append(a[-1].detach().requires_grad_())\n",
    "            a.append(self.node_labels.SecondForward(remote_a[-1]))\n",
    "            logits = a[-1]\n",
    "\n",
    "\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100*correct/total\n",
    "        # print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "        return  test_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [04:42<00:00, 28.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.39"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### datascientist.py\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_sizes = [128, 640]\n",
    "\n",
    "modelo_alice = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "modelo_bob = nn.Sequential(\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "optimizador_alice = optim.SGD(modelo_alice.parameters(), lr=0.03)\n",
    "optimizador_bob   = optim.SGD(modelo_alice.parameters(), lr=0.03)\n",
    "\n",
    "\n",
    "modelo = SplitNN_v1(\n",
    "    alice, modelo_alice, optimizador_alice,\n",
    "    bob, modelo_bob, optimizador_bob\n",
    ")\n",
    "\n",
    "modelo.test() # -> float del loss en el dataset entero\n",
    "modelo.train(n_epochs=10) # -> None\n",
    "modelo.test() # -> float del loss en el dataset entero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
